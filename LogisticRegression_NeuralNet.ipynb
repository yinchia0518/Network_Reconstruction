{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import pylab as pl\n",
    "import math\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_values_df = pd.read_csv('/Users/yinchiahuang/Library/Mobile Documents/com~apple~CloudDocs/RA/ML_data/z_values.csv')\n",
    "\n",
    "#2000\n",
    "banks_2000_df = pd.read_csv('/Users/yinchiahuang/Library/Mobile Documents/com~apple~CloudDocs/RA/ML_data/banks_2000.csv')\n",
    "companies_2000_df = pd.read_csv('/Users/yinchiahuang/Library/Mobile Documents/com~apple~CloudDocs/RA/ML_data/companies_2000.csv')\n",
    "connections_2000_df= pd.read_csv('/Users/yinchiahuang/Library/Mobile Documents/com~apple~CloudDocs/RA/ML_data/connections_2000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict of (bank:asset)\n",
    "def bank_dic_generator(banks_df):\n",
    "    bank_dic = pd.Series(banks_df['Assets'].values,index = banks_df['Bank']).to_dict()\n",
    "    bank_dic = dict((str(k), v) for k, v in bank_dic.items())\n",
    "    return bank_dic\n",
    "\n",
    "# create dict of (company:liability)\n",
    "def company_dic_generator(companies_df):\n",
    "    company_dic = pd.Series(companies_df['Liabilities'].values,index = companies_df['Company']).to_dict()\n",
    "    company_dic = dict((str(k), v) for k, v in company_dic.items())\n",
    "    return company_dic\n",
    "\n",
    "# create dict of (year:z)\n",
    "z_dic = pd.Series(z_values_df['z'].values,index = z_values_df['Year']).to_dict()\n",
    "z_dic = dict((str(k), v) for k, v in z_dic.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack connections_df  \n",
    "def stack_data(connections_df):\n",
    "    df = connections_df.copy()\n",
    "    df = df.set_index( 'Company' ) \n",
    "    df = df.stack() \n",
    "    df.index = df.index.rename( 'Bank' , level= 1 )\n",
    "    df.name = 'Connection (y)'\n",
    "    df = df.reset_index()\n",
    "    df['Company'] = df['Company'].astype(str)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variables \n",
    "def training_data_generator(connections_df,banks_df,companies_df ):\n",
    "    df = stack_data(connections_df)\n",
    "    bank_dic = bank_dic_generator(banks_df)\n",
    "    company_dic = company_dic_generator(companies_df)\n",
    "    df['Company Liabilities (x1)'] = df['Company'].map(company_dic)\n",
    "    df['Bank Assets (x2)']= df['Bank'].map(bank_dic)\n",
    "    df['x1*x2 (x3)'] = df.apply(lambda row: row['Company Liabilities (x1)'] * row['Bank Assets (x2)'], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Bank</th>\n",
       "      <th>Connection (y)</th>\n",
       "      <th>Company Liabilities (x1)</th>\n",
       "      <th>Bank Assets (x2)</th>\n",
       "      <th>x1*x2 (x3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>31597</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.008177</td>\n",
       "      <td>0.000111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>70001</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>5.558135</td>\n",
       "      <td>0.075363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>70002</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>2.187671</td>\n",
       "      <td>0.029663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>70003</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.000258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>70004</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>4.422931</td>\n",
       "      <td>0.059971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Company   Bank  Connection (y)  Company Liabilities (x1)  Bank Assets (x2)  \\\n",
       "0       1  31597               0                  0.013559          0.008177   \n",
       "1       1  70001               0                  0.013559          5.558135   \n",
       "2       1  70002               1                  0.013559          2.187671   \n",
       "3       1  70003               0                  0.013559          0.019000   \n",
       "4       1  70004               0                  0.013559          4.422931   \n",
       "\n",
       "   x1*x2 (x3)  \n",
       "0    0.000111  \n",
       "1    0.075363  \n",
       "2    0.029663  \n",
       "3    0.000258  \n",
       "4    0.059971  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2000 = training_data_generator(connections_2000_df,banks_2000_df,companies_2000_df)\n",
    "df_2000.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "The logistic regression model is one member of the supervised classification algorithm family. The building block concepts of logistic regression can be helpful in deep learning while building the neural networks. Logistic regression classifier is more like a linear classifier which uses the calculated logits (score ) to predict the target class. <br>\n",
    "In the mathematical side, the logistic regression model will pass the likelihood occurrences through the logistic function to predict the corresponding target class. This popular logistic function is the Softmax function. <br>\n",
    "To predict the probability of connections( whether a company and a bank will have a connection) given the features(company liabilties, bank assets, and the above two multiplied), we just need to multiply the feature score and the corresponding weight to get the score. The calculated score is also known as the logits.<br>\n",
    "The logit (Score) will pass into the softmax function to get the probability for each target class. In our case, if we pass the logit through the softmax function will get the probability for the target class(1 or 0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculute total cross entropy\n",
    "def CrossEntropy_caculator(yHats, ys):\n",
    "    CrossEntropy = 0\n",
    "    try:\n",
    "        for i in range(len(ys)):\n",
    "            if ys[i]==1:\n",
    "                CrossEntropy -= math.log(yHats[i])\n",
    "            else:\n",
    "                CrossEntropy-=math.log(1 - yHats[i])\n",
    "    except ValueError:\n",
    "        CrossEntropy = None\n",
    "    return CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticRegression(df_year):\n",
    "    df_year['intercept'] = 1.0\n",
    "    train_cols = df_year.columns[3:]\n",
    "    X = df_year[train_cols]\n",
    "    y = df_year['Connection (y)']\n",
    "\n",
    "    logit = sm.Logit(y, X)\n",
    "    result = logit.fit()\n",
    "\n",
    "    # predict ys\n",
    "    df_year['LogiReg'] = result.predict(X)    \n",
    "    del df_year['intercept']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.167766\n",
      "         Iterations 8\n"
     ]
    }
   ],
   "source": [
    "logisticRegression(df_2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Defined function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myFunction(x1x2, z):\n",
    "    return z*x1x2 / (1.+z*x1x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2000.loc[:, \"myfunc\"] = df_2000.loc[:, \"x1*x2 (x3)\"].apply(myFunction, args=(z_dic[\"2000\"],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Bank</th>\n",
       "      <th>Connection (y)</th>\n",
       "      <th>Company Liabilities (x1)</th>\n",
       "      <th>Bank Assets (x2)</th>\n",
       "      <th>x1*x2 (x3)</th>\n",
       "      <th>LogiReg</th>\n",
       "      <th>myfunc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>31597</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.008177</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.024735</td>\n",
       "      <td>0.002922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>70001</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>5.558135</td>\n",
       "      <td>0.075363</td>\n",
       "      <td>0.781942</td>\n",
       "      <td>0.665775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>70002</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>2.187671</td>\n",
       "      <td>0.029663</td>\n",
       "      <td>0.150586</td>\n",
       "      <td>0.439476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>70003</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.024969</td>\n",
       "      <td>0.006763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>70004</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>4.422931</td>\n",
       "      <td>0.059971</td>\n",
       "      <td>0.565673</td>\n",
       "      <td>0.613175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Company   Bank  Connection (y)  Company Liabilities (x1)  Bank Assets (x2)  \\\n",
       "0       1  31597               0                  0.013559          0.008177   \n",
       "1       1  70001               0                  0.013559          5.558135   \n",
       "2       1  70002               1                  0.013559          2.187671   \n",
       "3       1  70003               0                  0.013559          0.019000   \n",
       "4       1  70004               0                  0.013559          4.422931   \n",
       "\n",
       "   x1*x2 (x3)   LogiReg    myfunc  \n",
       "0    0.000111  0.024735  0.002922  \n",
       "1    0.075363  0.781942  0.665775  \n",
       "2    0.029663  0.150586  0.439476  \n",
       "3    0.000258  0.024969  0.006763  \n",
       "4    0.059971  0.565673  0.613175  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2000.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1c3e797fd0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.scatter(x=df_2000.loc[:, \"NeuralNetwork1\"],y=df_2000.loc[:, \"myfunc\"],c=df_2000.loc[:, \"Bank Assets (x2)\"],cmap=\"plasma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network  (softmax regression)\n",
    "**Keras Model Introduction:**<br>\n",
    "` STEP1. Define Network`\n",
    "> - The model needs to know what input shape it should expect. For this reason, the first layer in a Sequential model (and only the first, because following layers can do automatic shape inference) needs to receive information about its input shape. \n",
    "- Some 2D layers, such as Dense, support the specification of their input shape via the argument input_dim.\n",
    "- Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting.\n",
    "\n",
    "\n",
    "` STEP2. Compile Network`\n",
    "> Before training a model, we need to configure the learning process, which is done via the compile method. It receives three arguments:\n",
    "- An optimizer - This could be the string identifier of an existing optimizer (such as rmsprop or adagrad), or an instance of the Optimizer class. In our case, we use 'adam'\n",
    "- A loss function - This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as categorical_crossentropy or mse), or it can be an objective function. \n",
    "- A list of metrics - For any classification problem we will want to set this to metrics=['accuracy']. A metric could be the string identifier of an existing metric or a custom metric function.\n",
    "\n",
    "` STEP3. Fit Network`\n",
    "> Keras models are trained on Numpy arrays of input data and labels. For training a model, we will typically use the fit function.\n",
    "\n",
    "` STEP4. Evaluate Network`<br>\n",
    "> - evaluate() will calculate the loss function and the metrics.\n",
    "- The loss function takes the current output and compares it with the expected/true result. It's a function supposed to be minimized. The less the loss, the closer your results are to the expected. This is the function from which the derivatives will be taken so the backpropagation algorithm can update the weights.\n",
    "\n",
    "` STEP5. Prediction`\n",
    "\n",
    "**Softmax function Introduction:**<br>\n",
    "Softmax function is the popular function to calculate the probabilities of the events. The other mathematical advantages of using the softmax function are the output range.  Softmax function output values are always in the range of (0, 1). The sum of the output values will always equal to the 1. The Softmax is also known as the normalized exponential function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuralNetwork_1(df_year):\n",
    "    \n",
    "    df = copy.deepcopy(df_year)\n",
    "    # Bring X and y into numpy format\n",
    "    X_train = df[['Company Liabilities (x1)','Bank Assets (x2)','x1*x2 (x3)']].as_matrix()\n",
    "    y_bare = df['Connection (y)'].as_matrix()\n",
    "\n",
    "    # input and output dimensions\n",
    "    input_dim=X_train.shape[1]\n",
    "    output_dim = len(set(y_bare)) \n",
    "    nb_classes = len(set(y_bare))\n",
    "\n",
    "    # categorical classification values\n",
    "    Y_train = np_utils.to_categorical(y_bare, nb_classes)\n",
    "\n",
    "    ### set up the deep neural net with Keras\n",
    "    # initiate model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add weights input_layer--hidden_layer 1\n",
    "    model.add(Dense(5, input_dim=input_dim, init='lecun_uniform', activation='relu')) \n",
    "    # add hidden_layer 2\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    # add hidden_layer 3\n",
    "    model.add(Dense(55, activation='relu'))\n",
    "\n",
    "    # output layer\n",
    "    model.add(Dense(output_dim, activation='relu'))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    nb_epoch=60\n",
    "    batch_size=20\n",
    "    \n",
    "    ### compile model\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    model.fit(X_train, Y_train, nb_epoch=nb_epoch, batch_size=batch_size, verbose=False)\n",
    "    print (\"loss value & metrics values = \",model.evaluate(X_train, Y_train, batch_size=batch_size, verbose=False))\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuralNetwork_2(df_year):\n",
    "    df = copy.deepcopy(df_year)\n",
    "    \n",
    "    # Bring X and y into numpy format\n",
    "    X_train = df[['Company Liabilities (x1)','Bank Assets (x2)','x1*x2 (x3)']].as_matrix()\n",
    "    y_bare = df['Connection (y)'].as_matrix()\n",
    "\n",
    "    # input and output dimensions\n",
    "    input_dim=X_train.shape[1]\n",
    "    output_dim = len(set(y_bare)) \n",
    "    nb_classes = len(set(y_bare))\n",
    "\n",
    "    # categorical classification values\n",
    "    Y_train = np_utils.to_categorical(y_bare, nb_classes)\n",
    "\n",
    "    ### set up the deep neural net with Keras\n",
    "    # initiate model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add weights input_layer--hidden_layer 1\n",
    "    model.add(Dense(15, input_dim=input_dim, init='lecun_uniform', activation='relu')) \n",
    "    # add hidden_layer 2\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    # add hidden_layer 3\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    # add hidden_layer 4\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "\n",
    "    # output layer\n",
    "    model.add(Dense(output_dim, activation='relu'))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    nb_epoch = 80\n",
    "    batch_size=10\n",
    "    \n",
    "    ### compile model\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    model.fit(X_train, Y_train, nb_epoch=nb_epoch, batch_size=batch_size, verbose=False)\n",
    "    print (\"loss value & metrics values = \",model.evaluate(X_train, Y_train, batch_size=batch_size, verbose=False))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuralNetwork_3(df_year):\n",
    "    df = copy.deepcopy(df_year)\n",
    "    \n",
    "    # Bring X and y into numpy format\n",
    "    X_train = df[['Company Liabilities (x1)','Bank Assets (x2)','x1*x2 (x3)']].as_matrix()\n",
    "    y_bare = df['Connection (y)'].as_matrix()\n",
    "\n",
    "    # input and output dimensions\n",
    "    input_dim=X_train.shape[1]\n",
    "    output_dim = len(set(y_bare)) \n",
    "    nb_classes = len(set(y_bare))\n",
    "\n",
    "    # categorical classification values\n",
    "    Y_train = np_utils.to_categorical(y_bare, nb_classes)\n",
    "    \n",
    "    # initiate model\n",
    "    model = Sequential()\n",
    "    # add weights input_layer--hidden_layer 1\n",
    "    model.add(Dense(10, input_dim=input_dim, init='lecun_uniform', activation='relu')) \n",
    "    # add hidden_layer 2\n",
    "    model.add(Dense(5, activation='relu'))\n",
    "    # add output_layer\n",
    "    model.add(Dense(output_dim, activation='relu'))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    nb_epoch=50\n",
    "    batch_size=10\n",
    "    \n",
    "    ### compile model\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    model.fit(X_train, Y_train, nb_epoch=nb_epoch, batch_size=batch_size, verbose=False)\n",
    "    print (\"loss value & metrics values = \",model.evaluate(X_train, Y_train, batch_size=batch_size, verbose=False))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuralNetwork_4(df_year):\n",
    "    df = copy.deepcopy(df_year)\n",
    "    \n",
    "    # Bring X and y into numpy format\n",
    "    X_train = df[['Company Liabilities (x1)','Bank Assets (x2)','x1*x2 (x3)']].as_matrix()\n",
    "    y_bare = df['Connection (y)'].as_matrix()\n",
    "\n",
    "    # input and output dimensions\n",
    "    input_dim=X_train.shape[1]\n",
    "    output_dim = len(set(y_bare)) \n",
    "    nb_classes = len(set(y_bare))\n",
    "\n",
    "    # categorical classification values\n",
    "    Y_train = np_utils.to_categorical(y_bare, nb_classes)\n",
    "    \n",
    "    # initiate model\n",
    "    model = Sequential()\n",
    "    # add weights input_layer--hidden_layer 1\n",
    "    model.add(Dense(5, input_dim=input_dim, init='lecun_uniform', activation='relu')) \n",
    "    # add hidden_layer 2\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    # add hidden_layer 3\n",
    "    model.add(Dense(6, activation='relu'))\n",
    "    # add hidden_layer 4\n",
    "    model.add(Dense(3, activation='relu'))\n",
    "    # add output_layer\n",
    "    model.add(Dense(output_dim, activation='relu'))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    nb_epoch= 30\n",
    "    batch_size= 15\n",
    "    \n",
    "    ### compile model\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    model.fit(X_train, Y_train, nb_epoch=nb_epoch, batch_size=batch_size, verbose=False)\n",
    "    print (\"loss value & metrics values = \",model.evaluate(X_train, Y_train, batch_size=batch_size, verbose=False))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuralNetwork_5(df_year):\n",
    "    df = copy.deepcopy(df_year)\n",
    "    \n",
    "    # Bring X and y into numpy format\n",
    "    X_train = df[['Company Liabilities (x1)','Bank Assets (x2)','x1*x2 (x3)']].as_matrix()\n",
    "    y_bare = df['Connection (y)'].as_matrix()\n",
    "\n",
    "    # input and output dimensions\n",
    "    input_dim=X_train.shape[1]\n",
    "    output_dim = len(set(y_bare)) \n",
    "    nb_classes = len(set(y_bare))\n",
    "\n",
    "    # categorical classification values\n",
    "    Y_train = np_utils.to_categorical(y_bare, nb_classes)\n",
    "    \n",
    "    # initiate model\n",
    "    model = Sequential()\n",
    "    # add weights input_layer--hidden_layer 1\n",
    "    model.add(Dense(10, input_dim=input_dim, init='lecun_uniform', activation='relu')) \n",
    "    # add hidden_layer 2\n",
    "    model.add(Dense(3, activation='relu'))\n",
    "    # add hidden_layer 3\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    # add hidden_layer 4\n",
    "    model.add(Dense(5, activation='relu'))\n",
    "    # add hidden_layer 5\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    # add output_layer\n",
    "    model.add(Dense(output_dim, activation='relu'))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    nb_epoch= 90\n",
    "    batch_size= 5\n",
    "    \n",
    "    ### compile model\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    model.fit(X_train, Y_train, nb_epoch=nb_epoch, batch_size=batch_size, verbose=False)\n",
    "    print (\"loss value & metrics values = \",model.evaluate(X_train, Y_train, batch_size=batch_size, verbose=False))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuralNetwork_6(df_year):\n",
    "    df = copy.deepcopy(df_year)\n",
    "    \n",
    "    # Bring X and y into numpy format\n",
    "    X_train = df[['Company Liabilities (x1)','Bank Assets (x2)','x1*x2 (x3)']].as_matrix()\n",
    "    y_bare = df['Connection (y)'].as_matrix()\n",
    "\n",
    "    # input and output dimensions\n",
    "    input_dim=X_train.shape[1]\n",
    "    output_dim = len(set(y_bare)) \n",
    "    nb_classes = len(set(y_bare))\n",
    "\n",
    "    # categorical classification values\n",
    "    Y_train = np_utils.to_categorical(y_bare, nb_classes)\n",
    "    \n",
    "    # initiate model\n",
    "    model = Sequential()\n",
    "    # add weights input_layer--hidden_layer 1\n",
    "    model.add(Dense(13, input_dim=input_dim, init='lecun_uniform', activation='relu')) \n",
    "    # add hidden_layer 2\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    # add hidden_layer 3\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "\n",
    "    # add output_layer\n",
    "    model.add(Dense(output_dim, activation='relu'))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    nb_epoch= 200\n",
    "    batch_size= 15\n",
    "    \n",
    "    ### compile model\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    model.fit(X_train, Y_train, nb_epoch=nb_epoch, batch_size=batch_size, verbose=False)\n",
    "    print (\"loss value & metrics values = \",model.evaluate(X_train, Y_train, batch_size=batch_size, verbose=False))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n",
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(5, input_dim=3, activation=\"relu\", kernel_initializer=\"lecun_uniform\")`\n",
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:36: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value & metrics values =  [0.14777618688863253, 0.9444146072402694]\n"
     ]
    }
   ],
   "source": [
    "NN_model_1 = NeuralNetwork_1(df_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n",
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(15, input_dim=3, activation=\"relu\", kernel_initializer=\"lecun_uniform\")`\n",
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:38: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value & metrics values =  [0.14812573313849592, 0.9441020586320884]\n"
     ]
    }
   ],
   "source": [
    "NN_model_2 = NeuralNetwork_2(df_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n",
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(10, input_dim=3, activation=\"relu\", kernel_initializer=\"lecun_uniform\")`\n",
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value & metrics values =  [0.14906321211228432, 0.9392745999143601]\n"
     ]
    }
   ],
   "source": [
    "NN_model_3 = NeuralNetwork_3(df_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n",
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(5, input_dim=3, activation=\"relu\", kernel_initializer=\"lecun_uniform\")`\n",
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value & metrics values =  [0.14790538777259657, 0.9443663104238785]\n"
     ]
    }
   ],
   "source": [
    "NN_model_4 = NeuralNetwork_4(df_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n",
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(10, input_dim=3, activation=\"relu\", kernel_initializer=\"lecun_uniform\")`\n",
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:37: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value & metrics values =  [0.15022469244626332, 0.9445538391559085]\n"
     ]
    }
   ],
   "source": [
    "NN_model_5 = NeuralNetwork_5(df_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n",
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(13, input_dim=3, activation=\"relu\", kernel_initializer=\"lecun_uniform\")`\n",
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value & metrics values =  [0.14833852604334183, 0.9442128773656581]\n"
     ]
    }
   ],
   "source": [
    "NN_model_6 = NeuralNetwork_6(df_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Predictions\n",
    "def NN_predictions_generator(NN_model,df_year,model_number):\n",
    "    X_train = df_year[['Company Liabilities (x1)','Bank Assets (x2)','x1*x2 (x3)']].as_matrix()\n",
    "    predictions = NN_model.predict_proba(X_train)\n",
    "    yHats = [element[1] for element in predictions.tolist()]\n",
    "    column_name = 'NeuralNetwork'+str(model_number)\n",
    "    df_year[column_name] = yHats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "NN_predictions_generator(NN_model_1,df_2000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "NN_predictions_generator(NN_model_2,df_2000,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "NN_predictions_generator(NN_model_3,df_2000,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "NN_predictions_generator(NN_model_4,df_2000,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "NN_predictions_generator(NN_model_5,df_2000,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yinchiahuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "NN_predictions_generator(NN_model_6,df_2000,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x1c3d287d30>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1c3df73710>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_2000[['NeuralNetwork1','NeuralNetwork2']].hist(bins=30,log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Bank</th>\n",
       "      <th>Connection (y)</th>\n",
       "      <th>Company Liabilities (x1)</th>\n",
       "      <th>Bank Assets (x2)</th>\n",
       "      <th>x1*x2 (x3)</th>\n",
       "      <th>LogiReg</th>\n",
       "      <th>myfunc</th>\n",
       "      <th>NeuralNetwork1</th>\n",
       "      <th>NeuralNetwork2</th>\n",
       "      <th>NeuralNetwork3</th>\n",
       "      <th>NeuralNetwork4</th>\n",
       "      <th>NeuralNetwork5</th>\n",
       "      <th>NeuralNetwork6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>31597</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.008177</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.024735</td>\n",
       "      <td>0.002922</td>\n",
       "      <td>0.008332</td>\n",
       "      <td>0.008305</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>0.007403</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.004613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>70001</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>5.558135</td>\n",
       "      <td>0.075363</td>\n",
       "      <td>0.781942</td>\n",
       "      <td>0.665775</td>\n",
       "      <td>0.656373</td>\n",
       "      <td>0.634879</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.623853</td>\n",
       "      <td>0.621388</td>\n",
       "      <td>0.604521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>70002</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>2.187671</td>\n",
       "      <td>0.029663</td>\n",
       "      <td>0.150586</td>\n",
       "      <td>0.439476</td>\n",
       "      <td>0.320899</td>\n",
       "      <td>0.297914</td>\n",
       "      <td>0.352146</td>\n",
       "      <td>0.306472</td>\n",
       "      <td>0.286555</td>\n",
       "      <td>0.278511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>70003</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.024969</td>\n",
       "      <td>0.006763</td>\n",
       "      <td>0.016469</td>\n",
       "      <td>0.013065</td>\n",
       "      <td>0.008805</td>\n",
       "      <td>0.013819</td>\n",
       "      <td>0.005612</td>\n",
       "      <td>0.009633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>70004</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>4.422931</td>\n",
       "      <td>0.059971</td>\n",
       "      <td>0.565673</td>\n",
       "      <td>0.613175</td>\n",
       "      <td>0.588128</td>\n",
       "      <td>0.600445</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.603783</td>\n",
       "      <td>0.661258</td>\n",
       "      <td>0.576887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>70005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>3.926595</td>\n",
       "      <td>0.053241</td>\n",
       "      <td>0.455469</td>\n",
       "      <td>0.584591</td>\n",
       "      <td>0.557008</td>\n",
       "      <td>0.551016</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.547936</td>\n",
       "      <td>0.621241</td>\n",
       "      <td>0.527514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>70006</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>3.482018</td>\n",
       "      <td>0.047213</td>\n",
       "      <td>0.360030</td>\n",
       "      <td>0.555147</td>\n",
       "      <td>0.528740</td>\n",
       "      <td>0.477979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.496823</td>\n",
       "      <td>0.541110</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>70007</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>4.890041</td>\n",
       "      <td>0.066304</td>\n",
       "      <td>0.663954</td>\n",
       "      <td>0.636702</td>\n",
       "      <td>0.616791</td>\n",
       "      <td>0.614756</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.623853</td>\n",
       "      <td>0.684134</td>\n",
       "      <td>0.620641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>70008</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>1.889132</td>\n",
       "      <td>0.025615</td>\n",
       "      <td>0.119585</td>\n",
       "      <td>0.403716</td>\n",
       "      <td>0.310440</td>\n",
       "      <td>0.277831</td>\n",
       "      <td>0.291791</td>\n",
       "      <td>0.284363</td>\n",
       "      <td>0.266408</td>\n",
       "      <td>0.269302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>70009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>3.612588</td>\n",
       "      <td>0.048983</td>\n",
       "      <td>0.387284</td>\n",
       "      <td>0.564218</td>\n",
       "      <td>0.537069</td>\n",
       "      <td>0.494161</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.511876</td>\n",
       "      <td>0.570264</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>70010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>3.483752</td>\n",
       "      <td>0.047236</td>\n",
       "      <td>0.360386</td>\n",
       "      <td>0.555270</td>\n",
       "      <td>0.528851</td>\n",
       "      <td>0.478181</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.497023</td>\n",
       "      <td>0.541499</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>70011</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>1.334483</td>\n",
       "      <td>0.018094</td>\n",
       "      <td>0.076477</td>\n",
       "      <td>0.323533</td>\n",
       "      <td>0.242456</td>\n",
       "      <td>0.242815</td>\n",
       "      <td>0.226691</td>\n",
       "      <td>0.246210</td>\n",
       "      <td>0.231456</td>\n",
       "      <td>0.252711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>70012</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>2.375912</td>\n",
       "      <td>0.032215</td>\n",
       "      <td>0.173349</td>\n",
       "      <td>0.459901</td>\n",
       "      <td>0.327587</td>\n",
       "      <td>0.310997</td>\n",
       "      <td>0.383062</td>\n",
       "      <td>0.320894</td>\n",
       "      <td>0.299717</td>\n",
       "      <td>0.286498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>70017</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.133145</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>0.027572</td>\n",
       "      <td>0.045545</td>\n",
       "      <td>0.059282</td>\n",
       "      <td>0.054869</td>\n",
       "      <td>0.043389</td>\n",
       "      <td>0.048445</td>\n",
       "      <td>0.034791</td>\n",
       "      <td>0.050485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>70018</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.025237</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.025105</td>\n",
       "      <td>0.008964</td>\n",
       "      <td>0.023126</td>\n",
       "      <td>0.016943</td>\n",
       "      <td>0.012089</td>\n",
       "      <td>0.019262</td>\n",
       "      <td>0.009357</td>\n",
       "      <td>0.014657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>70019</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.022580</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.025047</td>\n",
       "      <td>0.008028</td>\n",
       "      <td>0.020018</td>\n",
       "      <td>0.015169</td>\n",
       "      <td>0.010843</td>\n",
       "      <td>0.016969</td>\n",
       "      <td>0.007528</td>\n",
       "      <td>0.012277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>70020</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.030128</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.025212</td>\n",
       "      <td>0.010682</td>\n",
       "      <td>0.025859</td>\n",
       "      <td>0.020756</td>\n",
       "      <td>0.014765</td>\n",
       "      <td>0.023275</td>\n",
       "      <td>0.012303</td>\n",
       "      <td>0.020155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>70021</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.011311</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.024802</td>\n",
       "      <td>0.004037</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.009472</td>\n",
       "      <td>0.004440</td>\n",
       "      <td>0.008873</td>\n",
       "      <td>0.002982</td>\n",
       "      <td>0.005711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>70022</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.010415</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.024783</td>\n",
       "      <td>0.003719</td>\n",
       "      <td>0.009761</td>\n",
       "      <td>0.009122</td>\n",
       "      <td>0.004099</td>\n",
       "      <td>0.008426</td>\n",
       "      <td>0.002770</td>\n",
       "      <td>0.005373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>70023</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.021172</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.025016</td>\n",
       "      <td>0.007531</td>\n",
       "      <td>0.018541</td>\n",
       "      <td>0.014304</td>\n",
       "      <td>0.010235</td>\n",
       "      <td>0.015654</td>\n",
       "      <td>0.006707</td>\n",
       "      <td>0.011161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>70024</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.033618</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.025288</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.026987</td>\n",
       "      <td>0.023979</td>\n",
       "      <td>0.017022</td>\n",
       "      <td>0.026626</td>\n",
       "      <td>0.014681</td>\n",
       "      <td>0.021389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>70025</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.024595</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.004934</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>0.005080</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>0.002959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>70026</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.182688</td>\n",
       "      <td>0.002477</td>\n",
       "      <td>0.028782</td>\n",
       "      <td>0.061451</td>\n",
       "      <td>0.071193</td>\n",
       "      <td>0.065942</td>\n",
       "      <td>0.051821</td>\n",
       "      <td>0.059601</td>\n",
       "      <td>0.044619</td>\n",
       "      <td>0.059442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>70027</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.042960</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.025495</td>\n",
       "      <td>0.015163</td>\n",
       "      <td>0.030248</td>\n",
       "      <td>0.035192</td>\n",
       "      <td>0.024867</td>\n",
       "      <td>0.030546</td>\n",
       "      <td>0.020082</td>\n",
       "      <td>0.023629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>70028</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.295218</td>\n",
       "      <td>0.004003</td>\n",
       "      <td>0.031725</td>\n",
       "      <td>0.095681</td>\n",
       "      <td>0.100844</td>\n",
       "      <td>0.099170</td>\n",
       "      <td>0.077041</td>\n",
       "      <td>0.090835</td>\n",
       "      <td>0.077590</td>\n",
       "      <td>0.085571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>70029</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.160116</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>0.028224</td>\n",
       "      <td>0.054270</td>\n",
       "      <td>0.065379</td>\n",
       "      <td>0.060660</td>\n",
       "      <td>0.047802</td>\n",
       "      <td>0.054247</td>\n",
       "      <td>0.039850</td>\n",
       "      <td>0.055190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>70030</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.372662</td>\n",
       "      <td>0.005053</td>\n",
       "      <td>0.033918</td>\n",
       "      <td>0.117823</td>\n",
       "      <td>0.115592</td>\n",
       "      <td>0.130027</td>\n",
       "      <td>0.100490</td>\n",
       "      <td>0.119168</td>\n",
       "      <td>0.092827</td>\n",
       "      <td>0.109211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>70031</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.007807</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.024727</td>\n",
       "      <td>0.002790</td>\n",
       "      <td>0.008116</td>\n",
       "      <td>0.008177</td>\n",
       "      <td>0.003248</td>\n",
       "      <td>0.007247</td>\n",
       "      <td>0.002235</td>\n",
       "      <td>0.004498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>70032</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.022981</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.025055</td>\n",
       "      <td>0.008169</td>\n",
       "      <td>0.020460</td>\n",
       "      <td>0.015424</td>\n",
       "      <td>0.011022</td>\n",
       "      <td>0.017363</td>\n",
       "      <td>0.007779</td>\n",
       "      <td>0.012615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>70033</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.260166</td>\n",
       "      <td>0.003528</td>\n",
       "      <td>0.030778</td>\n",
       "      <td>0.085289</td>\n",
       "      <td>0.094736</td>\n",
       "      <td>0.087474</td>\n",
       "      <td>0.068169</td>\n",
       "      <td>0.079944</td>\n",
       "      <td>0.065447</td>\n",
       "      <td>0.076476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351915</th>\n",
       "      <td>91581</td>\n",
       "      <td>70117</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.035099</td>\n",
       "      <td>0.001986</td>\n",
       "      <td>0.027760</td>\n",
       "      <td>0.049874</td>\n",
       "      <td>0.036104</td>\n",
       "      <td>0.039070</td>\n",
       "      <td>0.023990</td>\n",
       "      <td>0.040387</td>\n",
       "      <td>0.021183</td>\n",
       "      <td>0.033016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351916</th>\n",
       "      <td>91581</td>\n",
       "      <td>70118</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.039259</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>0.027873</td>\n",
       "      <td>0.055457</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>0.044579</td>\n",
       "      <td>0.028710</td>\n",
       "      <td>0.046106</td>\n",
       "      <td>0.026213</td>\n",
       "      <td>0.034768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351917</th>\n",
       "      <td>91581</td>\n",
       "      <td>70119</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.029245</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>0.027602</td>\n",
       "      <td>0.041904</td>\n",
       "      <td>0.033526</td>\n",
       "      <td>0.032413</td>\n",
       "      <td>0.018608</td>\n",
       "      <td>0.033096</td>\n",
       "      <td>0.015567</td>\n",
       "      <td>0.029158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351918</th>\n",
       "      <td>91581</td>\n",
       "      <td>70120</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.020415</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.027364</td>\n",
       "      <td>0.029627</td>\n",
       "      <td>0.023612</td>\n",
       "      <td>0.024401</td>\n",
       "      <td>0.012656</td>\n",
       "      <td>0.021398</td>\n",
       "      <td>0.009696</td>\n",
       "      <td>0.016529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351919</th>\n",
       "      <td>91581</td>\n",
       "      <td>70121</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.012834</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.027162</td>\n",
       "      <td>0.018832</td>\n",
       "      <td>0.014780</td>\n",
       "      <td>0.018291</td>\n",
       "      <td>0.006616</td>\n",
       "      <td>0.013745</td>\n",
       "      <td>0.006255</td>\n",
       "      <td>0.009767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351920</th>\n",
       "      <td>91581</td>\n",
       "      <td>70123</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.026511</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.027528</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.032384</td>\n",
       "      <td>0.029693</td>\n",
       "      <td>0.016519</td>\n",
       "      <td>0.029691</td>\n",
       "      <td>0.013448</td>\n",
       "      <td>0.024479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351921</th>\n",
       "      <td>91581</td>\n",
       "      <td>70127</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.028621</td>\n",
       "      <td>0.001619</td>\n",
       "      <td>0.027585</td>\n",
       "      <td>0.041047</td>\n",
       "      <td>0.033262</td>\n",
       "      <td>0.031772</td>\n",
       "      <td>0.018110</td>\n",
       "      <td>0.032287</td>\n",
       "      <td>0.015056</td>\n",
       "      <td>0.028019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351922</th>\n",
       "      <td>91581</td>\n",
       "      <td>70128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.011968</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.027139</td>\n",
       "      <td>0.017584</td>\n",
       "      <td>0.013983</td>\n",
       "      <td>0.017657</td>\n",
       "      <td>0.006118</td>\n",
       "      <td>0.013064</td>\n",
       "      <td>0.005799</td>\n",
       "      <td>0.009196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351923</th>\n",
       "      <td>91581</td>\n",
       "      <td>70129</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.026839</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.005180</td>\n",
       "      <td>0.005675</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>0.006685</td>\n",
       "      <td>0.002138</td>\n",
       "      <td>0.004152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351924</th>\n",
       "      <td>91581</td>\n",
       "      <td>70130</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.003985</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.026928</td>\n",
       "      <td>0.005924</td>\n",
       "      <td>0.007224</td>\n",
       "      <td>0.008528</td>\n",
       "      <td>0.002970</td>\n",
       "      <td>0.008170</td>\n",
       "      <td>0.002881</td>\n",
       "      <td>0.005267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351925</th>\n",
       "      <td>91581</td>\n",
       "      <td>70133</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.038990</td>\n",
       "      <td>0.002206</td>\n",
       "      <td>0.027866</td>\n",
       "      <td>0.055098</td>\n",
       "      <td>0.037921</td>\n",
       "      <td>0.044201</td>\n",
       "      <td>0.028379</td>\n",
       "      <td>0.045714</td>\n",
       "      <td>0.025856</td>\n",
       "      <td>0.034652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351926</th>\n",
       "      <td>91581</td>\n",
       "      <td>70135</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.026838</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>0.005164</td>\n",
       "      <td>0.005653</td>\n",
       "      <td>0.002175</td>\n",
       "      <td>0.006672</td>\n",
       "      <td>0.002132</td>\n",
       "      <td>0.004142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351927</th>\n",
       "      <td>91581</td>\n",
       "      <td>70136</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.007954</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.027033</td>\n",
       "      <td>0.011756</td>\n",
       "      <td>0.010230</td>\n",
       "      <td>0.013688</td>\n",
       "      <td>0.004255</td>\n",
       "      <td>0.010321</td>\n",
       "      <td>0.004081</td>\n",
       "      <td>0.006951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351928</th>\n",
       "      <td>91581</td>\n",
       "      <td>70137</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.007192</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.027013</td>\n",
       "      <td>0.010641</td>\n",
       "      <td>0.009624</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.003971</td>\n",
       "      <td>0.009868</td>\n",
       "      <td>0.003817</td>\n",
       "      <td>0.006591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351929</th>\n",
       "      <td>91581</td>\n",
       "      <td>70138</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.025325</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.027496</td>\n",
       "      <td>0.036492</td>\n",
       "      <td>0.031639</td>\n",
       "      <td>0.028583</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.028322</td>\n",
       "      <td>0.012621</td>\n",
       "      <td>0.022684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351930</th>\n",
       "      <td>91581</td>\n",
       "      <td>70139</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.005514</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.026969</td>\n",
       "      <td>0.008179</td>\n",
       "      <td>0.008386</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>0.003411</td>\n",
       "      <td>0.008940</td>\n",
       "      <td>0.003295</td>\n",
       "      <td>0.005862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351931</th>\n",
       "      <td>91581</td>\n",
       "      <td>70140</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.009262</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.027068</td>\n",
       "      <td>0.013662</td>\n",
       "      <td>0.011359</td>\n",
       "      <td>0.015811</td>\n",
       "      <td>0.004790</td>\n",
       "      <td>0.011145</td>\n",
       "      <td>0.004576</td>\n",
       "      <td>0.007615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351932</th>\n",
       "      <td>91581</td>\n",
       "      <td>70141</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.014175</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.027198</td>\n",
       "      <td>0.020759</td>\n",
       "      <td>0.016062</td>\n",
       "      <td>0.019317</td>\n",
       "      <td>0.007468</td>\n",
       "      <td>0.014868</td>\n",
       "      <td>0.006930</td>\n",
       "      <td>0.010723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351933</th>\n",
       "      <td>91581</td>\n",
       "      <td>70142</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.019210</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>0.027332</td>\n",
       "      <td>0.027927</td>\n",
       "      <td>0.021924</td>\n",
       "      <td>0.023470</td>\n",
       "      <td>0.011754</td>\n",
       "      <td>0.019949</td>\n",
       "      <td>0.009088</td>\n",
       "      <td>0.015207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351934</th>\n",
       "      <td>91581</td>\n",
       "      <td>70144</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.009676</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.027079</td>\n",
       "      <td>0.014264</td>\n",
       "      <td>0.011742</td>\n",
       "      <td>0.016080</td>\n",
       "      <td>0.004973</td>\n",
       "      <td>0.011420</td>\n",
       "      <td>0.004745</td>\n",
       "      <td>0.007838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351935</th>\n",
       "      <td>91581</td>\n",
       "      <td>70145</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.241866</td>\n",
       "      <td>0.013685</td>\n",
       "      <td>0.033955</td>\n",
       "      <td>0.265633</td>\n",
       "      <td>0.115258</td>\n",
       "      <td>0.158329</td>\n",
       "      <td>0.137460</td>\n",
       "      <td>0.127755</td>\n",
       "      <td>0.107564</td>\n",
       "      <td>0.127095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351936</th>\n",
       "      <td>91581</td>\n",
       "      <td>70146</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.105212</td>\n",
       "      <td>0.005953</td>\n",
       "      <td>0.029727</td>\n",
       "      <td>0.135955</td>\n",
       "      <td>0.072307</td>\n",
       "      <td>0.097676</td>\n",
       "      <td>0.068244</td>\n",
       "      <td>0.080304</td>\n",
       "      <td>0.073615</td>\n",
       "      <td>0.062725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351937</th>\n",
       "      <td>91581</td>\n",
       "      <td>70147</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.026868</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>0.005777</td>\n",
       "      <td>0.006485</td>\n",
       "      <td>0.002413</td>\n",
       "      <td>0.007139</td>\n",
       "      <td>0.002357</td>\n",
       "      <td>0.004488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351938</th>\n",
       "      <td>91581</td>\n",
       "      <td>70148</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.026851</td>\n",
       "      <td>0.001544</td>\n",
       "      <td>0.005414</td>\n",
       "      <td>0.005990</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>0.006865</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>0.004285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351939</th>\n",
       "      <td>91581</td>\n",
       "      <td>70149</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.026847</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>0.005335</td>\n",
       "      <td>0.005883</td>\n",
       "      <td>0.002241</td>\n",
       "      <td>0.006804</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0.004240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351940</th>\n",
       "      <td>91581</td>\n",
       "      <td>70150</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.009448</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.027073</td>\n",
       "      <td>0.013933</td>\n",
       "      <td>0.011529</td>\n",
       "      <td>0.015931</td>\n",
       "      <td>0.004871</td>\n",
       "      <td>0.011268</td>\n",
       "      <td>0.004651</td>\n",
       "      <td>0.007714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351941</th>\n",
       "      <td>91581</td>\n",
       "      <td>70151</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.006368</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.026991</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.009008</td>\n",
       "      <td>0.011333</td>\n",
       "      <td>0.003686</td>\n",
       "      <td>0.009401</td>\n",
       "      <td>0.003551</td>\n",
       "      <td>0.006222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351942</th>\n",
       "      <td>91581</td>\n",
       "      <td>70154</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.001525</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.002275</td>\n",
       "      <td>0.005680</td>\n",
       "      <td>0.006353</td>\n",
       "      <td>0.002376</td>\n",
       "      <td>0.007067</td>\n",
       "      <td>0.002322</td>\n",
       "      <td>0.004435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351943</th>\n",
       "      <td>91581</td>\n",
       "      <td>70156</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.010291</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.027095</td>\n",
       "      <td>0.015157</td>\n",
       "      <td>0.012333</td>\n",
       "      <td>0.016489</td>\n",
       "      <td>0.005257</td>\n",
       "      <td>0.011840</td>\n",
       "      <td>0.005008</td>\n",
       "      <td>0.008182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351944</th>\n",
       "      <td>91581</td>\n",
       "      <td>70157</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.026857</td>\n",
       "      <td>0.001881</td>\n",
       "      <td>0.005535</td>\n",
       "      <td>0.006155</td>\n",
       "      <td>0.002319</td>\n",
       "      <td>0.006957</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>0.004353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>351945 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Company   Bank  Connection (y)  Company Liabilities (x1)  \\\n",
       "0            1  31597               0                  0.013559   \n",
       "1            1  70001               0                  0.013559   \n",
       "2            1  70002               1                  0.013559   \n",
       "3            1  70003               0                  0.013559   \n",
       "4            1  70004               0                  0.013559   \n",
       "5            1  70005               0                  0.013559   \n",
       "6            1  70006               0                  0.013559   \n",
       "7            1  70007               1                  0.013559   \n",
       "8            1  70008               0                  0.013559   \n",
       "9            1  70009               0                  0.013559   \n",
       "10           1  70010               0                  0.013559   \n",
       "11           1  70011               1                  0.013559   \n",
       "12           1  70012               1                  0.013559   \n",
       "13           1  70017               0                  0.013559   \n",
       "14           1  70018               0                  0.013559   \n",
       "15           1  70019               0                  0.013559   \n",
       "16           1  70020               0                  0.013559   \n",
       "17           1  70021               0                  0.013559   \n",
       "18           1  70022               0                  0.013559   \n",
       "19           1  70023               0                  0.013559   \n",
       "20           1  70024               0                  0.013559   \n",
       "21           1  70025               0                  0.013559   \n",
       "22           1  70026               0                  0.013559   \n",
       "23           1  70027               0                  0.013559   \n",
       "24           1  70028               0                  0.013559   \n",
       "25           1  70029               0                  0.013559   \n",
       "26           1  70030               0                  0.013559   \n",
       "27           1  70031               0                  0.013559   \n",
       "28           1  70032               0                  0.013559   \n",
       "29           1  70033               0                  0.013559   \n",
       "...        ...    ...             ...                       ...   \n",
       "351915   91581  70117               0                  0.056580   \n",
       "351916   91581  70118               0                  0.056580   \n",
       "351917   91581  70119               0                  0.056580   \n",
       "351918   91581  70120               0                  0.056580   \n",
       "351919   91581  70121               0                  0.056580   \n",
       "351920   91581  70123               0                  0.056580   \n",
       "351921   91581  70127               0                  0.056580   \n",
       "351922   91581  70128               0                  0.056580   \n",
       "351923   91581  70129               0                  0.056580   \n",
       "351924   91581  70130               0                  0.056580   \n",
       "351925   91581  70133               0                  0.056580   \n",
       "351926   91581  70135               0                  0.056580   \n",
       "351927   91581  70136               0                  0.056580   \n",
       "351928   91581  70137               0                  0.056580   \n",
       "351929   91581  70138               0                  0.056580   \n",
       "351930   91581  70139               0                  0.056580   \n",
       "351931   91581  70140               0                  0.056580   \n",
       "351932   91581  70141               0                  0.056580   \n",
       "351933   91581  70142               0                  0.056580   \n",
       "351934   91581  70144               0                  0.056580   \n",
       "351935   91581  70145               0                  0.056580   \n",
       "351936   91581  70146               0                  0.056580   \n",
       "351937   91581  70147               0                  0.056580   \n",
       "351938   91581  70148               0                  0.056580   \n",
       "351939   91581  70149               0                  0.056580   \n",
       "351940   91581  70150               0                  0.056580   \n",
       "351941   91581  70151               0                  0.056580   \n",
       "351942   91581  70154               0                  0.056580   \n",
       "351943   91581  70156               0                  0.056580   \n",
       "351944   91581  70157               0                  0.056580   \n",
       "\n",
       "        Bank Assets (x2)  x1*x2 (x3)   LogiReg    myfunc  NeuralNetwork1  \\\n",
       "0               0.008177    0.000111  0.024735  0.002922        0.008332   \n",
       "1               5.558135    0.075363  0.781942  0.665775        0.656373   \n",
       "2               2.187671    0.029663  0.150586  0.439476        0.320899   \n",
       "3               0.019000    0.000258  0.024969  0.006763        0.016469   \n",
       "4               4.422931    0.059971  0.565673  0.613175        0.588128   \n",
       "5               3.926595    0.053241  0.455469  0.584591        0.557008   \n",
       "6               3.482018    0.047213  0.360030  0.555147        0.528740   \n",
       "7               4.890041    0.066304  0.663954  0.636702        0.616791   \n",
       "8               1.889132    0.025615  0.119585  0.403716        0.310440   \n",
       "9               3.612588    0.048983  0.387284  0.564218        0.537069   \n",
       "10              3.483752    0.047236  0.360386  0.555270        0.528851   \n",
       "11              1.334483    0.018094  0.076477  0.323533        0.242456   \n",
       "12              2.375912    0.032215  0.173349  0.459901        0.327587   \n",
       "13              0.133145    0.001805  0.027572  0.045545        0.059282   \n",
       "14              0.025237    0.000342  0.025105  0.008964        0.023126   \n",
       "15              0.022580    0.000306  0.025047  0.008028        0.020018   \n",
       "16              0.030128    0.000409  0.025212  0.010682        0.025859   \n",
       "17              0.011311    0.000153  0.024802  0.004037        0.010400   \n",
       "18              0.010415    0.000141  0.024783  0.003719        0.009761   \n",
       "19              0.021172    0.000287  0.025016  0.007531        0.018541   \n",
       "20              0.033618    0.000456  0.025288  0.011905        0.026987   \n",
       "21              0.001673    0.000023  0.024595  0.000599        0.004934   \n",
       "22              0.182688    0.002477  0.028782  0.061451        0.071193   \n",
       "23              0.042960    0.000582  0.025495  0.015163        0.030248   \n",
       "24              0.295218    0.004003  0.031725  0.095681        0.100844   \n",
       "25              0.160116    0.002171  0.028224  0.054270        0.065379   \n",
       "26              0.372662    0.005053  0.033918  0.117823        0.115592   \n",
       "27              0.007807    0.000106  0.024727  0.002790        0.008116   \n",
       "28              0.022981    0.000312  0.025055  0.008169        0.020460   \n",
       "29              0.260166    0.003528  0.030778  0.085289        0.094736   \n",
       "...                  ...         ...       ...       ...             ...   \n",
       "351915          0.035099    0.001986  0.027760  0.049874        0.036104   \n",
       "351916          0.039259    0.002221  0.027873  0.055457        0.038050   \n",
       "351917          0.029245    0.001655  0.027602  0.041904        0.033526   \n",
       "351918          0.020415    0.001155  0.027364  0.029627        0.023612   \n",
       "351919          0.012834    0.000726  0.027162  0.018832        0.014780   \n",
       "351920          0.026511    0.001500  0.027528  0.038136        0.032384   \n",
       "351921          0.028621    0.001619  0.027585  0.041047        0.033262   \n",
       "351922          0.011968    0.000677  0.027139  0.017584        0.013983   \n",
       "351923          0.000583    0.000033  0.026839  0.000871        0.005180   \n",
       "351924          0.003985    0.000225  0.026928  0.005924        0.007224   \n",
       "351925          0.038990    0.002206  0.027866  0.055098        0.037921   \n",
       "351926          0.000550    0.000031  0.026838  0.000822        0.005164   \n",
       "351927          0.007954    0.000450  0.027033  0.011756        0.010230   \n",
       "351928          0.007192    0.000407  0.027013  0.010641        0.009624   \n",
       "351929          0.025325    0.001433  0.027496  0.036492        0.031639   \n",
       "351930          0.005514    0.000312  0.026969  0.008179        0.008386   \n",
       "351931          0.009262    0.000524  0.027068  0.013662        0.011359   \n",
       "351932          0.014175    0.000802  0.027198  0.020759        0.016062   \n",
       "351933          0.019210    0.001087  0.027332  0.027927        0.021924   \n",
       "351934          0.009676    0.000547  0.027079  0.014264        0.011742   \n",
       "351935          0.241866    0.013685  0.033955  0.265633        0.115258   \n",
       "351936          0.105212    0.005953  0.029727  0.135955        0.072307   \n",
       "351937          0.001697    0.000096  0.026868  0.002531        0.005777   \n",
       "351938          0.001034    0.000059  0.026851  0.001544        0.005414   \n",
       "351939          0.000883    0.000050  0.026847  0.001319        0.005335   \n",
       "351940          0.009448    0.000535  0.027073  0.013933        0.011529   \n",
       "351941          0.006368    0.000360  0.026991  0.009434        0.009008   \n",
       "351942          0.001525    0.000086  0.026864  0.002275        0.005680   \n",
       "351943          0.010291    0.000582  0.027095  0.015157        0.012333   \n",
       "351944          0.001260    0.000071  0.026857  0.001881        0.005535   \n",
       "\n",
       "        NeuralNetwork2  NeuralNetwork3  NeuralNetwork4  NeuralNetwork5  \\\n",
       "0             0.008305        0.003357        0.007403        0.002304   \n",
       "1             0.634879        0.500000        0.623853        0.621388   \n",
       "2             0.297914        0.352146        0.306472        0.286555   \n",
       "3             0.013065        0.008805        0.013819        0.005612   \n",
       "4             0.600445        0.500000        0.603783        0.661258   \n",
       "5             0.551016        0.500000        0.547936        0.621241   \n",
       "6             0.477979        0.500000        0.496823        0.541110   \n",
       "7             0.614756        0.500000        0.623853        0.684134   \n",
       "8             0.277831        0.291791        0.284363        0.266408   \n",
       "9             0.494161        0.500000        0.511876        0.570264   \n",
       "10            0.478181        0.500000        0.497023        0.541499   \n",
       "11            0.242815        0.226691        0.246210        0.231456   \n",
       "12            0.310997        0.383062        0.320894        0.299717   \n",
       "13            0.054869        0.043389        0.048445        0.034791   \n",
       "14            0.016943        0.012089        0.019262        0.009357   \n",
       "15            0.015169        0.010843        0.016969        0.007528   \n",
       "16            0.020756        0.014765        0.023275        0.012303   \n",
       "17            0.009472        0.004440        0.008873        0.002982   \n",
       "18            0.009122        0.004099        0.008426        0.002770   \n",
       "19            0.014304        0.010235        0.015654        0.006707   \n",
       "20            0.023979        0.017022        0.026626        0.014681   \n",
       "21            0.004035        0.001877        0.005080        0.001348   \n",
       "22            0.065942        0.051821        0.059601        0.044619   \n",
       "23            0.035192        0.024867        0.030546        0.020082   \n",
       "24            0.099170        0.077041        0.090835        0.077590   \n",
       "25            0.060660        0.047802        0.054247        0.039850   \n",
       "26            0.130027        0.100490        0.119168        0.092827   \n",
       "27            0.008177        0.003248        0.007247        0.002235   \n",
       "28            0.015424        0.011022        0.017363        0.007779   \n",
       "29            0.087474        0.068169        0.079944        0.065447   \n",
       "...                ...             ...             ...             ...   \n",
       "351915        0.039070        0.023990        0.040387        0.021183   \n",
       "351916        0.044579        0.028710        0.046106        0.026213   \n",
       "351917        0.032413        0.018608        0.033096        0.015567   \n",
       "351918        0.024401        0.012656        0.021398        0.009696   \n",
       "351919        0.018291        0.006616        0.013745        0.006255   \n",
       "351920        0.029693        0.016519        0.029691        0.013448   \n",
       "351921        0.031772        0.018110        0.032287        0.015056   \n",
       "351922        0.017657        0.006118        0.013064        0.005799   \n",
       "351923        0.005675        0.002181        0.006685        0.002138   \n",
       "351924        0.008528        0.002970        0.008170        0.002881   \n",
       "351925        0.044201        0.028379        0.045714        0.025856   \n",
       "351926        0.005653        0.002175        0.006672        0.002132   \n",
       "351927        0.013688        0.004255        0.010321        0.004081   \n",
       "351928        0.012502        0.003971        0.009868        0.003817   \n",
       "351929        0.028583        0.015686        0.028322        0.012621   \n",
       "351930        0.010236        0.003411        0.008940        0.003295   \n",
       "351931        0.015811        0.004790        0.011145        0.004576   \n",
       "351932        0.019317        0.007468        0.014868        0.006930   \n",
       "351933        0.023470        0.011754        0.019949        0.009088   \n",
       "351934        0.016080        0.004973        0.011420        0.004745   \n",
       "351935        0.158329        0.137460        0.127755        0.107564   \n",
       "351936        0.097676        0.068244        0.080304        0.073615   \n",
       "351937        0.006485        0.002413        0.007139        0.002357   \n",
       "351938        0.005990        0.002272        0.006865        0.002224   \n",
       "351939        0.005883        0.002241        0.006804        0.002195   \n",
       "351940        0.015931        0.004871        0.011268        0.004651   \n",
       "351941        0.011333        0.003686        0.009401        0.003551   \n",
       "351942        0.006353        0.002376        0.007067        0.002322   \n",
       "351943        0.016489        0.005257        0.011840        0.005008   \n",
       "351944        0.006155        0.002319        0.006957        0.002269   \n",
       "\n",
       "        NeuralNetwork6  \n",
       "0             0.004613  \n",
       "1             0.604521  \n",
       "2             0.278511  \n",
       "3             0.009633  \n",
       "4             0.576887  \n",
       "5             0.527514  \n",
       "6             0.500000  \n",
       "7             0.620641  \n",
       "8             0.269302  \n",
       "9             0.500000  \n",
       "10            0.500000  \n",
       "11            0.252711  \n",
       "12            0.286498  \n",
       "13            0.050485  \n",
       "14            0.014657  \n",
       "15            0.012277  \n",
       "16            0.020155  \n",
       "17            0.005711  \n",
       "18            0.005373  \n",
       "19            0.011161  \n",
       "20            0.021389  \n",
       "21            0.002959  \n",
       "22            0.059442  \n",
       "23            0.023629  \n",
       "24            0.085571  \n",
       "25            0.055190  \n",
       "26            0.109211  \n",
       "27            0.004498  \n",
       "28            0.012615  \n",
       "29            0.076476  \n",
       "...                ...  \n",
       "351915        0.033016  \n",
       "351916        0.034768  \n",
       "351917        0.029158  \n",
       "351918        0.016529  \n",
       "351919        0.009767  \n",
       "351920        0.024479  \n",
       "351921        0.028019  \n",
       "351922        0.009196  \n",
       "351923        0.004152  \n",
       "351924        0.005267  \n",
       "351925        0.034652  \n",
       "351926        0.004142  \n",
       "351927        0.006951  \n",
       "351928        0.006591  \n",
       "351929        0.022684  \n",
       "351930        0.005862  \n",
       "351931        0.007615  \n",
       "351932        0.010723  \n",
       "351933        0.015207  \n",
       "351934        0.007838  \n",
       "351935        0.127095  \n",
       "351936        0.062725  \n",
       "351937        0.004488  \n",
       "351938        0.004285  \n",
       "351939        0.004240  \n",
       "351940        0.007714  \n",
       "351941        0.006222  \n",
       "351942        0.004435  \n",
       "351943        0.008182  \n",
       "351944        0.004353  \n",
       "\n",
       "[351945 rows x 14 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2000.to_csv('results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "CE_Logi = CrossEntropy_caculator(df_2000['LogiReg'].tolist(),df_2000['Connection (y)'].tolist())\n",
    "CE_myfunc = CrossEntropy_caculator(df_2000['myfunc'].tolist(),df_2000['Connection (y)'].tolist())\n",
    "CE_NN1 = CrossEntropy_caculator(df_2000['NeuralNetwork1'].tolist(),df_2000['Connection (y)'].tolist())\n",
    "CE_NN2 = CrossEntropy_caculator(df_2000['NeuralNetwork2'].tolist(),df_2000['Connection (y)'].tolist())\n",
    "CE_NN3 = CrossEntropy_caculator(df_2000['NeuralNetwork3'].tolist(),df_2000['Connection (y)'].tolist())\n",
    "CE_NN4 = CrossEntropy_caculator(df_2000['NeuralNetwork4'].tolist(),df_2000['Connection (y)'].tolist())\n",
    "CE_NN5 = CrossEntropy_caculator(df_2000['NeuralNetwork5'].tolist(),df_2000['Connection (y)'].tolist())\n",
    "CE_NN6 = CrossEntropy_caculator(df_2000['NeuralNetwork6'].tolist(),df_2000['Connection (y)'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy\n",
      "==============================\n",
      "LogiReg       : 59044.5239555\n",
      "myfunc        : 58354.76126841183\n",
      "NeuralNetwork1: 52009.092842017526\n",
      "NeuralNetwork2: 52132.11394370863\n",
      "NeuralNetwork3: 52462.05469865061\n",
      "NeuralNetwork4: 52054.56436471885\n",
      "NeuralNetwork5: 52870.83210994489\n",
      "NeuralNetwork6: None\n"
     ]
    }
   ],
   "source": [
    "print('Cross Entropy')\n",
    "print('==============================')\n",
    "print ('LogiReg       :' ,CE_Logi)\n",
    "print ('myfunc        :' ,CE_myfunc)\n",
    "print ('NeuralNetwork1:' ,CE_NN1)\n",
    "print ('NeuralNetwork2:' ,CE_NN2)\n",
    "print ('NeuralNetwork3:' ,CE_NN3)\n",
    "print ('NeuralNetwork4:' ,CE_NN4)\n",
    "print ('NeuralNetwork5:' ,CE_NN5)\n",
    "print ('NeuralNetwork6:' ,CE_NN6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg     :  21372.000000000004\n",
      "My function:  21372.000216851036\n",
      "Neural net1:  21641.9776554954\n",
      "Neural net2:  22266.2343294156\n",
      "Neural net3:  20317.731610149553\n",
      "Neural net4:  22220.39771875809\n",
      "Neural net5:  18464.18850111562\n",
      "Neural net6:  20306.8918625719\n"
     ]
    }
   ],
   "source": [
    "print(\"LogReg     : \", np.sum(df_2000.loc[:, \"LogiReg\"]))\n",
    "print(\"My function: \", np.sum(df_2000.loc[:, \"myfunc\"]))\n",
    "print(\"Neural net1: \", np.sum(df_2000.loc[:, \"NeuralNetwork1\"]))\n",
    "print(\"Neural net2: \", np.sum(df_2000.loc[:, \"NeuralNetwork2\"]))\n",
    "print(\"Neural net3: \", np.sum(df_2000.loc[:, \"NeuralNetwork3\"]))\n",
    "print(\"Neural net4: \", np.sum(df_2000.loc[:, \"NeuralNetwork4\"]))\n",
    "print(\"Neural net5: \", np.sum(df_2000.loc[:, \"NeuralNetwork5\"]))\n",
    "print(\"Neural net6: \", np.sum(df_2000.loc[:, \"NeuralNetwork6\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "http://dataaspirant.com/2017/03/14/multinomial-logistic-regression-model-works-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
